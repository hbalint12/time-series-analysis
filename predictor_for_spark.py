# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1SInYY4zpb1ONx2ZaXhoE_bKZiNMobLHi
"""
import time
import pandas as pd
from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
from pyspark import SparkContext, SparkConf
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand
from keras.initializers import glorot_uniform
import keras
import math
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential, load_model
from keras.layers import Dense, LSTM
import tensorflow as tf
import matplotlib.pyplot as plt

plt.style.use('fivethirtyeight')

conf = SparkConf().setAppName('predictor').setMaster('spark://192.168.56.5:7077')
sc = SparkContext(conf=conf)

sparkSession = SparkSession.builder.appName("predictor").getOrCreate()

start_time = time.time()

schema = StructType([
    StructField("Date", TimestampType()),
    StructField("Open", FloatType()),
    StructField("High", FloatType()),
    StructField("Low", FloatType()),
    StructField("Close", FloatType()),
    StructField("Adj Close", FloatType()),
    StructField("Volume", FloatType())
])

df = sparkSession.read.csv('hdfs://192.168.56.5:9000/EURHUF.csv', header=True, schema=schema)
df = df.dropna()
df.show()
log4jLogger = sc._jvm.org.apache.log4j
LOGGER = log4jLogger.LogManager.getLogger(__name__)
LOGGER.info("Data read")

data = df.select('Close')
df.unpersist(blocking=True)
data_pd = data.toPandas()
data.unpersist()
plt.figure(figsize=(16, 8))
plt.title('Close Price History')
plt.plot(data_pd)
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price', fontsize=18)
plt.show()

dataset = np.array(data_pd)
training_data_len = math.ceil(len(dataset) * .8)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)
train_data = scaled_data[0:training_data_len, :]
x_train = []
y_train = []
for i in range(30, len(train_data)):
    x_train.append(train_data[i - 30:i, 0])
    y_train.append(train_data[i, 0])

x_train, y_train = np.array(x_train), np.array(y_train)
x_train.shape

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

model = Sequential()
model.add(LSTM(205, return_sequences=False, input_shape=(x_train.shape[1], 1)))
model.add(Dense(1, activation='relu'))
model.compile(optimizer='adam', loss='mean_squared_error')
LOGGER.info('Model compiled')
rdd = to_simple_rdd(sc, x_train, y_train)
spark_model = SparkModel(model, frequency='batch', mode='hogwild', num_workers=2)
LOGGER.info('Spark model created')
spark_model.fit(rdd, epochs=50, batch_size=500, validation_split=0.01)
LOGGER.info('Spark model trained')
LOGGER.info(model.summary())
test_data = scaled_data[training_data_len - 30:, :]
x_test = []
y_test = dataset[training_data_len:, :]

y_test = dataset[training_data_len:, :]
for i in range(30, len(test_data)):
    x_test.append(test_data[i - 30:i, 0])

x_test = np.array(x_test)

x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

predictions = spark_model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))
rmse

train = data_pd[:training_data_len]
valid = data_pd[training_data_len:]
valid['Predictions'] = predictions
plt.figure(figsize=(16, 8))
plt.title('Model prediction on EUR/HUF')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price EUR/HUF', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')
LOGGER.info("--- %s seconds ---" % (time.time() - start_time))
LOGGER.info('RMSE: ' + str(rmse))
plt.show()
